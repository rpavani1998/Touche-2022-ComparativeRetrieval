for example, sorting transparent objects before rendering or sorting objects by state attribute (e.g. texture) to improve batching.
// nothing to sort if(count 20k arrays using merge sort (the speed-ups with quicksort are not as good).
this can be done with the __offload keyword and a simple queue: template void sortchunkqueued(t *data, size_t start, size_t end, threadqueue *queue) { size_t count = (end - start + 1); if(count waitforslot(); // execute the block on a spu and immediately return an execution handle offloadthread_t handle = __offload(data, start, end, count) { size_t size
blocks the function we use to offload sorting on 1 spu (sortoffloadedlocalmem) is blocking, which means that the ppu is blocked while the spu is sorting the array.
doing two sorting operations on different spus at the same time is then quite easy: t *array1, array2; size_t size1, size2; threadqueue queue; sortqueued(array1, 0, size1 - 1, &queue); sortqueued(array2, 0, size2 - 1, &queue); queue.joinallthreads(); now that we can execute code on multiple spus at the same time, we can start designing our parallel sort implementation.
in simple cases an array of handles could be used, but threadqueue is easier to use with recursive functions.
threadqueue queue; sortqueued(array1, 0, size1 - 1, &queue); sortqueued(array2, 0, size2 - 1, &queue); queue.joinallthreads(); parallel sort on 2 spus now that we can execute code on multiple spus at the same time, we can start designing our parallel sort implementation.
with multi-core architectures like the ps3 it makes sense to parallelize this operation to maximize the use of these cores.
in the next and last post in the series we will see how to improve our parallel sort so that it can run on 4 spus instead of 2.
the easiest solution would be to divide the array into chunks that can fit in spu memory and then merge them on the ppu.
(t *)memalign(16, count * sizeof(t)); if(!temp) { failed_malloc("temp"); return; } parallelsort(data, temp, 0, count - 1, &queue); free(temp); } this implementation behaves identically to the sequential one, except that when the size of the array gets small enough, sorting is offloaded on a spu.
in the previous part we have started running these implementations on a spu to improve performance even further.
in simple cases an array of handles could be used, but threadqueue is easier to use with recursive functions.
in addition, despite using the vectorized merge function presented earlier in the series, we do not see any significant difference in performance between sorting floats on integers on the spu.
the function we use to offload sorting on 1 spu (sortoffloadedlocalmem) is blocking, which means that the ppu is blocked while the spu is sorting the array.
can be used to enqueue multiple blocks and join (i.e. wait for) them afterwards.
the easiest solution would be to divide the array into chunks that can fit in spu memory and then merge them on the ppu.
in this part we will run our sort functions on multiple spus and create a parallel sort implementation.
since there are two recursive calls in parallelsort and the blocks have to be joined right after that, at most two blocks will be executing in parallel on spus.
in addition, we will show how to parallelize merge to run on any number of spus which will improve performance even further.
because of the way most algorithms work (comparing and swapping pairs of items) sorting often takes precious time.
the threadqueue class (not part of offload) can be used to enqueue multiple blocks and join (i.e. wait for) them afterwards.
