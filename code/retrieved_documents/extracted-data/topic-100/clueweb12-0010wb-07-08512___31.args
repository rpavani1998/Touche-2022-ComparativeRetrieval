for example, the entire financial system hinges on trust, and the level of trust in financial instruments is not known to be particularly stable.
five years ago, in what is web 2.0, tim o'reilly said that "data is the next intel inside."
all of this data would be useless if we couldn't store it, and that's where moore's law comes in.
the explosion of text data (distinct from audio, video, etc.) is wholly due to kiddies such as google who didn't want to build bcnf datastores because they're (the kiddies, not the data) just too dense.
they accept all data formats, including the most messy, and their schemas evolve as the understanding of the data changes.
most recently discussed related topics related events strata 2012, february 28-march 1, santa clara, calif.
however, data from o'reilly research shows a steady year-over-year increase in hadoop and cassandra job listings, which are good proxies for the "data science" market as a whole.
however, data from o'reilly research shows a steady year-over-year increase in hadoop and cassandra job listings, which are good proxies for the "data science" market as a whole.
point-of-sale devices and frequent-shopper's cards make it possible to capture all of your retail transactions, not just the ones you make online.
the first step of any data analysis project is "data conditioning," or getting data into a state where it's usable.
save 30% on registration with the code stn11rad google is a master at creating data products.
these recommendations are "data products" that help to drive amazon's more traditional retail business.
many sources of "wild data" are extremely messy.
suitable for extremely large databases (billions of rows, millions of columns), distributed across thousands of nodes.
on the subject of visualization, stanford's protovis is also probably worthy of considerationhttp://vis.stanford.edu/protovis/ i've only started trying it out, but protovis is just javascript and svg, and that's a big plus for me.
yahoo's claim that they had built the world's largest production hadoop application, with 10,000 cores running linux, brought it onto center stage.
they aren't well-behaved xml files with all the metadata nicely in place.
the need to define a schema in advance conflicts with reality of multiple, unstructured data sources, in which you may not know what's important until after you've analyzed the data.
hadoop processes data as it arrives, and delivers intermediate results in (near) real-time.
in the "map" stage, a programming task is divided into a number of identical subtasks, which are then distributed across many processors; the intermediate results are then combined by a single reduce task.
machine learning is another essential tool for the data scientist.
data scientists data science requires skills ranging from traditional computer science to mathematics to art.
edward tufte'svisual display of quantitative information is the classic for data visualization, and a foundational text for anyone practicing data science.
it has a 5 mb capacity
data science isn't just about the existence of data, or making guesses about what that data might mean; it's about testing hypotheses and making sure that the conclusions you're drawing from the data are valid.
we are seeing more data in formats that are easier to consume: atom data feeds, web services, microformats, and other newer technologies provide data in formats that's directly machine-consumable.
the errors in the r and stat books are such to make it obvious your authors (you, too) have an understanding of inferential statistics (the only variety of any value) about as deep as the iso-9000 and six sigma folks.
i also am a bit skeptical about the fetish-izing of 'big data'.
i.e. bringing a pot of water to boil is not caused by switching on the stove.
it really encouraged and motivated me a lot as a newbie data scientist.
extracting information from data is more like cooking, an art.
faster computations make it easier to test different assumptions, different datasets, and different algorithms.
where data comes from working with data at scale making data tell its story data scientists download free pdf versionwe've all heard it: according to hal varian, statistics is the next sexy job.
usurp ones "observe" and "orient" functions in their closed-loop decision processes, and you can then predict an individual's "decide" and "act" part of the loop (as described by john boyd).
statistics further only produce correlation but not causation.
storing data is only part of building a data platform, though.
before it does anything else, itunes reads the length of every track, sends it to cddb, and gets back the track titles.
data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution.
if the problem involves human language, understanding the data adds another dimension to the problem.
scientists also know how to break large problems up into smaller problems.
it would have been easy to turn this into a high-ceremony development project that would take thousands of hours of developer time, plus thousands of hours of computing time to do massive correlations across linkedin's membership.
the thread that ties most of these applications together is that data collected from users provides added value.
megan [ 5 june 2010 01:33 pm] kudos for encouraging thoughtfulness about data, and interdisciplinary approaches to data analysis problems.
but hadoop (and particularly elastic mapreduce) make it easy to build clusters that can perform computations on long datasets quickly.
it's easer to consult with clients to figure out whether you're asking the right questions, and it's possible to pursue intriguing possibilities that you'd otherwise have to drop for lack of time.
the web is full of "data-driven apps."
the developers of cddb realized that any cd had a unique signature, based on the exact length (in samples) of each track on the cd.
we're increasingly finding data in the wild, and data scientists are involved with gathering data, massaging it into a tractable form, making it tell its story, and presenting that story to others.
there's a database behind a web front end, and middleware that talks to a number of other databases and data services (credit card processing companies, banks, and so on).
all of this data would be useless if we couldn't store it, and that's where moore's law comes in.
filtering the relevant stuff is not easy and mostly an intuitive human skill as long as the model can be grasped.
according to martin wattenberg (@wattenberg , founder offlowing media), visualization is key to data conditioning: if you want to find out just how bad your data is, try plotting it.
data science is cool.
storage, is indeed, cheap, and getting cheaper.
they group together fundamentally dissimilar products by telling you what they aren't.
tracking links made google searches much more useful, and pagerank has been a key ingredient to the company's success.
i'm aware that tools have been available to manipulate data for decades.
the article would be even better if it didn't contain simple calculation errors like "ram has moved from $1,000/mb to roughly $25/gb -- a price reduction of about 4000".
while i haven't stressed traditional statistics, building statistical models plays an important role in any data analysis.
in contrast, a 32 gb microsd card measures around 5/8 x 3/8 inch and weighs about 0.5 gram.
we don't yet know what those products are, but we do know that the winners will be the people, and the companies, that find those products.
but google has made huge strides by using the voice data they've collected, and has been able tointegrate voice search into their core search engine.
facebook and linkedin use patterns of friendship relationships to suggest other people you may know, or should know, with sometimes frightening accuracy.
near real-time data analysis enables features liketrending topics on sites like twitter.
you need some creativity for when the story the data is telling isn't what you think it's telling.
the turk is an excellent way to develop training sets.
this graph shows the increase in cassandra jobs, and the companies listing cassandra positions, over time.
am] your graph of job listings vs time displays no indication of its vertical scale.
once you've parsed the data, you can start thinking about the quality of your data.
according to martin wattenberg (@wattenberg , founder offlowing media), visualization is key to data conditioning: if you want to find out just how bad your data is, try plotting it.
i won't even go into the already mentioned issues of privacy and misuse of the collected data by governments and big business.
"data mashups in r" analyzes mortgage foreclosures in philadelphia county by taking a public report from the county sheriff's office, extracting addresses and using yahoo to convert the addresses to latitude and longitude, then using the geographical data to place the foreclosures on a map (another data source), and group them by neighborhood, valuation, neighborhood per-capita income, and other socio-economic factors.
data science isn't just about the existence of data, or making guesses about what that data might mean; it's about testing hypotheses and making sure that the conclusions you're drawing from the data are valid.
" also important is the ability to figure out how to tap into existing sources of data, rather than collecting your own.
they come about because amazon understands that a book isn't just a book, a camera isn't just a camera, and a customer isn't just a customer; customers generate a trail of "data exhaust" that can be mined and put to use, and a camera is a cloud of data that can be correlated with the customers' behavior, the data they leave every time they visit the site.
it incorporateshdfs, a distributed filesystem designed for the performance and reliability requirements of huge datasets; the hbase database;hive, which lets developers explore hadoop datasets using sql-like queries; a high-level dataflow language calledpig; and other components.
although r is an odd and quirky language, particularly to someone with a background in computer science, it comes close to providing "one stop shopping" for most statistical work.
the thread that ties most of these applications together is that data collected from users provides added value.
data science requires skills ranging from traditional computer science to mathematics to art.
according to hilary mason (@hmason), data scientist atbit.ly, it's possible to precompute much of the calculation, then use one of the experiments in real-time mapreduce to get presentable results.
the models of global warming and climate change are complete rubbish and so will be many of the data models that try to make sense of all the random data collected.
their business is fundamentally different from selling music, sharing music, or analyzing musical tastes (though these can also be "data products").
while rock-solid consistency is crucial to many applications, it's not really necessary for the kind of analysis we're discussing here.
vector [ 4 december 2010 10:46 am] thanks for the good and quite useful article.
1956 disk drive one of the first commercial disk drives from ibm.
tell us it's not relevant doesn't cut it.
try usinggoogle trends to figure out what's happening with thecassandra database or the python language, and you'll get a sense of the problem.
whether you look at bits per gram, bits per dollar, or raw capacity, storage has more than kept pace with the increase of cpu speed.
sites likeinfochimps and factual provide access to many large datasets, including climate data, myspace activity streams, and game logs from sporting events.
amazon'selastic mapreduce makes it much easier to put hadoop to work without investing in racks of linux machines, by providing preconfigured hadoop images for its ec2 clusters.
here's a few examples: google's breakthrough was realizing that a search engine could use input other than the text on the page.
the need to define a schema in advance conflicts with reality of multiple, unstructured data sources, in which you may not know what's important until after you've analyzed the data.
roger magoulas, who runs the data analysis group at o'reilly, was recently searching a database for apple job listings requiring geolocation skills.
describing the data science group he put together at facebook (possibly the first data science group at a consumer-oriented web property), jeff hammerbacher said: ... on any given day, a team member could author a multistage processing pipeline in python, design a hypothesis test, perform a regression analysis over data samples with r, design and implement an algorithm for some data-intensive product or service in hadoop, or communicate the results of our analyses to other members of the organization3
data conditioning can involve cleaning up messy html with tools like beautiful soup, natural language processing to parse plain text in english and other languages, or even getting humans to do the dirty work.
it is made for high throughput batch processing, not quick turn around.
data analysis with open source tools this book shows you how to think about data and the results you want to achieve with it.
if you've ever used itunes to rip a cd, you've taken advantage of this database.
according to jeff hammerbacher2 (@hackingdata), we're trying to build information platforms or dataspaces.
the more storage is available, the more data you will find to put into it.
22-23, covers the latest and best tools and technologies for data science -- from gathering, cleaning, analyzing, and storing data to communicating data intelligence effectively.
according to hilary mason (@hmason), data scientist atbit.ly, it's possible to precompute much of the calculation, then use one of the experiments in real-time mapreduce to get presentable results.
as with the number of followers on twitter, a "trending topics" report only needs to be current to within five minutes -- or even an hour.
data is only useful if you can do something with it, and enormous datasets present computational problems.
for example, if you're looking at job listings, and want to know which originated with apple, you can have real people do the classification for roughly $0.01 each.
disambiguation is never an easy task, but tools like thenatural language toolkit library can make it simpler.
i'm a librarian and our lot faces a handful of data as well.
i've just fixed the reference to the ozone layer.
making data tell its story isn't just a matter of presenting results; it involves making connections, then going back to other data sources to verify them.
there was insufficient computing power, the data was all locked up in proprietary sources, and the tools for working with the data were insufficient.
it looks like the big companies such as google may know a little bit too much about us... rissy
provide data in formats that's directly machine-consumable.
i think by using mapreduce one can find out the hidden patterns in unstructured data.
data expands to fill the space you have to store it.
second, of course we have, or will soon have, the algorithms available to process this data.
we often need to pull content from external public websites (usually government and npo's), which don't have any form of exportable data formats.
let's not discard them with a flippant comment about actuaries and the false belief that anyone can run a few programs in r and correctly analyze data.
visualization is crucial to each stage of the data scientist.
the nasa article denies this, but also says that in 1984, they decided that the low values (whch went back to the 70s) were "real."
which is why managing a business with bpm workflows and predictive analytics is for idiots who don't understand nature and science.
you don't have to look at many modern web applications to see classification, error detection, image matching (behindgoogle goggles and snaptell) and even face detection -- an ill-advised mobile application lets you take someone's picture with a cell phone, and look up that person's identity using photos available online.
google has indexed many, many websites about large snakes.
suitable for extremely large databases (billions of rows, millions of columns), distributed across thousands of nodes.
you wouldn't use mapreduce like this in a low-latency query situation.
i think we need to be careful not to lose sight of the drivers of science and where much of science comes from - intuition and great ideas .. after which we create experiments to test hypotheses which inevitably involve data of some kind.
near real-time data analysis enables features liketrending topics on sites like twitter.
flu trends google was able to spot trends in the swine flu epidemic roughly two weeks before the center for disease control by analyzing searches that people were making in different regions of the country.
do if not process lots of (sensory) data extremely cleverly.
traditional data analysis has been hampered by extremely long turn-around times.
the value of data analysis and representation critically depends on access to trustworthy source data.
it has excellent graphics facilities; cran includes parsers for many kinds of data; and newer extensions extend r into distributed computing.
muhammad mudssar [ 3 june 2010 10:45 pm] good article about data and its its importance specially importance of massive unstructured data.
facebook and linkedin use patterns of friendship relationships to suggest other people you may know, or should know, with sometimes frightening accuracy.
so far it's worked out pretty well as we've been able to "scrape" raw data as structured information from hundreds of public sources.
the web has people spending more time online, and leaving a trail of data wherever they go.
there was insufficient computing power, the data was all locked up in proprietary sources, and the tools for working with the data were insufficient.
the data exhaust you leave behind whenever you surf the web, friend someone on facebook, or make a purchase in your local supermarket, is all carefully collected and analyzed.
if data is incongruous, do you decide that something is wrong with badly behaved data (after all, equipment fails), or that the incongruous data is telling its own story, which may be more interesting?
spell checking isn't a terribly difficult problem, but by suggesting corrections to misspelled searches, and observing what the user clicks in response, google made it much more accurate.
whether we're talking about web server logs, tweet streams, online transaction records, "citizen science," data from sensors, government data, or some other source, the problem isn't finding data, it's figuring out what to do with it.
they can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: "here's a lot of data, what can you make from it?" the future belongs to the companies who figure out how to collect and use data successfully.
whether it's mining your personal biology, building maps from the shared experience of millions of travellers, or studying the urls that people pass to others, the next generation of successful businesses will be built around data.
save 20% with code radar20 archives Â© 2012, o'reilly media, inc. (707) 827-7019(800) 889-8969 all trademarks and registered trademarks appearing on oreilly.com are the property of their respective owners.
more to the point, it's easy to notice that one advertisement forr in a nutshell generated 2 percent more conversions than another.
using data effectively requires something different from traditional statistics, where actuaries in business suits perform arcane but fairly well-defined kinds of analysis.
but old-stylescreen scraping hasn't died, and isn't going to die.
point-of-sale devices and frequent-shopper's cards make it possible to capture all of your retail transactions, not just the ones you make online.
they can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: "here's a lot of data, what can you make from it?" the future belongs to the companies who figure out how to collect and use data successfully.
in this post, i examine the many sides of data science -- the technologies, the companies and the unique skill sets.
this data was presented as an html file that was probably generated automatically from a spreadsheet.
it has a very flexible data model.
but google has made huge strides by using the voice data they've collected, and has been able tointegrate voice search into their core search engine.
of course data science is described as a sexy gig when you're bosses are control freaks.
information platforms are similar to traditional data warehouses, but different.
we are reaching a stage where the great limiting factor to significant progress is our intelligent processing systems, we are beginning to have the data but we don't yet have suitably developed systems to intelligently process that data (both in a human and non-human manner).
factual enlists users to update and improve its datasets, which cover topics as diverse as endocrinologists to hiking trails.
if data is incongruous, do you decide that something is wrong with badly behaved data (after all, equipment fails), or that the incongruous data is telling its own story, which may be more interesting?
increased storage capacity demands increased sophistication in the analysis and use of that data.
when natural language processing fails, you can replace artificial intelligence with human intelligence.
but the cddb staff used data creatively to solve a much more tractable problem that gave them the same result.
gnuplot is very effective; r incorporates a fairly comprehensive graphics package; casey reas' and ben fry'sprocessing is the state of the art, particularly if you need to create animations that show how things change over time.
beautiful data learn from the best data practitioners in the field about how wide-ranging -- and beautiful -- working with data can be.
the nasa article denies this, but also says that in 1984, they decided that the low values (whch went back to the 70s) were "real."
the developers of cddb realized that any cd had a unique signature, based on the exact length (in samples) of each track on the cd.
a friend of mine wrote an article that i think may be interesting to fellow readers: http://info.livelogic.net/customer-intelligence-project-success/bid/47060/pandora-s-jar-and-the-art-of-the-cross-sell thanks for the post.
while this sounds simple enough, it's revolutionary: cddb views music as data, not as audio, and creates new value in doing so.
hadoop goes far beyond a simple mapreduce implementation (of which there are several); it's the key component of a data platform.
they can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions.
during the swine flu epidemic of 2009, google was able to track the progress of the epidemicby following searches for flu-related topics.
"data mashups in r" analyzes mortgage foreclosures in philadelphia county by taking a public report from the county sheriff's office, extracting addresses and using yahoo to convert the addresses to latitude and longitude, then using the geographical data to place the foreclosures on a map (another data source), and group them by neighborhood, valuation, neighborhood per-capita income, and other socio-economic factors.
head first data analysis learn how to collect your data, sort the distractions from the truth, and find meaningful patterns.
for example, if you're looking at job listings, and want to know which originated with apple, you can have real people do the classification for roughly $0.01 each.
relational databases are designed for consistency, to support complex transactions that can easily be rolled back if any one of a complex set of operations fails.
the first step of any data analysis project is "data conditioning," or getting data into a state where it's usable.
i hope this movement will play a big role in the future as there is so much benefits to leverage data from the public and private sectors.
in contrast, a 32 gb microsd card measures around 5/8 x 3/8 inch and weighs about 0.5 gram.
whether we're talking about web server logs, tweet streams, online transaction records, "citizen science," data from sensors, government data, or some other source, the problem isn't finding data, it's figuring out what to do with it.
once you've collected your training data (perhaps a large collection of public photos from twitter), you can have humans classify them inexpensively -- possibly sorting them into categories, possibly drawing circles around faces, cars, or whatever interests you.
using data effectively requires something different from traditional statistics, where actuaries in business suits perform arcane but fairly well-defined kinds of analysis.
if you start a calculation, it might not finish for hours, or even days.
in an article on the importance of data, that leaves me wondering how much of the content is hype and how much is well thought out.
it's reported that the discovery of ozone layer depletion was delayed becauseautomated data collection tools discarded readings that were too low 1.
at some point, traditional techniques for working with data run out of steam.
and you're right, in finance there's a marketing industry that's designed to manufacture trust entirely aside from the quality of the data or the analysis.
the result was a valuable data product that analyzed a huge database -- but it was never conceived as such.
while i haven't stressed traditional statistics, building statistical models plays an important role in any data analysis.
you have to make it tell its story.
it's an excellent way to classify a few thousand data points at a cost of a few cents each.
yahoo's claim that they had built the world's largest production hadoop application, with 10,000 cores running linux, brought it onto center stage.
data analysis with open source tools this book shows you how to think about data and the results you want to achieve with it.
we converted it as a formidable tool and gave a face to understand it better by user's.
many of these databases are the logical descendants of google'sbigtable and amazon's dynamo, and are designed to be distributed across many nodes, to provide "eventual consistency" but not absolute consistency, and to have very flexible schema.
physicists have a strong mathematical background, computing skills, and come from a discipline in which survival depends on getting the most from the data.
g. boyd [ 6 june 2010 09:20 pm] data science exists in order to usurp an individual's ooda loop without their knowledge, as described by former air force pilot john boyd.
as to r, yes, it is useful, but sas still rules the job listings.
whether that data is search terms, voice samples, or product reviews, the users are in a feedback loop in which they contribute to the products they use.
these features only require soft real-time; reports on trending topics don't require millisecond accuracy.
many of the key hadoop developers have found a home atcloudera, which provides commercial support.
statistical processing does however not replicate reality, but is always built on a model illusion.
put out the feelers and collect more data (figure out what to do with it later).
beautiful visualization this book demonstrates why visualizations are beautiful not only for their aesthetic design, but also for elegant layers of detail.
according to jeff hammerbacher2 (@hackingdata), we're trying to build information platforms or dataspaces.
you don't have to look at many modern web applications to see classification, error detection, image matching (behindgoogle goggles and snaptell) and even face detection -- an ill-advised mobile application lets you take someone's picture with a cell phone, and look up that person's identity using photos available online.
if you've ever seen the html that's generated by excel, you know that's going to be fun to process.
brandyn [ 2 june 2010 08:06 pm] "in hindsight, mapreduce seems like an obvious solution to google's biggest problem, creating large searches.
that's an important insight: we're entering the era of products that are built on data.
mining large data sets to death is very similar to "spreadsheet-itis" that spread quickly in te 1980's once spreadsheets became widely available.
whether it's mining your personal biology, building maps from the shared experience of millions of travellers, or studying the urls that people pass to others, the next generation of successful businesses will be built around data.
the turk is an excellent way to develop training sets.
recently a meteorological link was discovered whereby a the increased uvb radiation due to the depleted ozone layer does affect air circulation patterns in the antarctic, but this is basically a reflection of the fact that almost any aspects of the earth system if you look hard enough.
so they build, using xml to compound the insult, massively redundant flat files, which in turn demand vast amounts of hdd and cpu to process.
according to dj patil, chief scientist atlinkedin (@dpatil), the best data scientists tend to be "hard scientists," particularly physicists, rather than computer science majors.
working with data at scale making data tell its story data scientists download free pdf version we've all heard it: according to hal varian, statistics is the next sexy job.
that's an important insight: we're entering the era of products that are built on data.
during the swine flu epidemic of 2009, google was able to track the progress of the epidemicby following searches for flu-related topics.
data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution.
it would be nice if there was a standard set of tools to do the job, but there isn't.
google'spagerank algorithm was among the first to use data outside of the page itself, in particular, the number of links pointing to a page.
to do it well you need to understand the grammatical structure of a job posting; you need to be able to parse the english.
that's where services like amazon's mechanical turk come in.
this data was presented as an html file that was probably generated automatically from a spreadsheet.
head first data analysis learn how to collect your data, sort the distractions from the truth, and find meaningful patterns.
nothing wrong with using spreadsheets, except that they tended to focus attention on the questions where data that was available, rather than the questions that really needed answers.
we're increasingly finding data in the wild, and data scientists are involved with gathering data, massaging it into a tractable form, making it tell its story, and presenting that story to others.
it was an agile, flexible process that built toward its goal incrementally, rather than tackling a huge mountain of data all at once.
gracenote built a database of track lengths, and coupled it to a database of album metadata (track titles, artists, album titles).
at o'reilly, we frequently combine publishing industry data fromnielsen bookscan with our own sales data, publicly available amazon data, and even job data to see what's happening in the publishing industry.
even a relatively large job only costs a few hundred dollars.
while rock-solid consistency is crucial to many applications, it's not really necessary for the kind of analysis we're discussing here.
the number of references & links to languages, platforms/frameworks, tools, and projects was very impressive!
her job as scientist at bit.ly is really to investigate the data that bit.ly is generating, and find out how to build interesting products from it.
then at books members had in their libraries.
the web has people spending more time online, and leaving a trail of data wherever they go.
in the "map" stage, a programming task is divided into a number of identical subtasks, which are then distributed across many processors; the intermediate results are then combined by a single reduce task.
yes, some of it will be useful as an aha moment, but it certainly won't predict the future or allow us to control things better.
hilary mason says that when she gets a new data set, she starts by making a dozen or more scatter plots, trying to get a sense of what might be interesting.
we don't yet know what those products are, but we do know that the winners will be the people, and the companies, that find those products.
certainly sometimes data sets are necessarily large, but arguably better data, acquired through better questions, is preferable.
managing sharding and replication across a horde of database servers is difficult and slow.
if you have a cd that's not in the database (including a cd you've made yourself), you can create an entry for an unknown album.
it incorporateshdfs, a distributed filesystem designed for the performance and reliability requirements of huge datasets; the hbase database;hive, which lets developers explore hadoop datasets using sql-like queries; a high-level dataflow language calledpig; and other components.
we've all heard the joke that eating pickles causes death, because everyone who dies has eaten pickles.
according tomike driscoll ( @dataspora), statistics is the "grammar of data science."
what differentiates data science from statistics is that data science is a holistic approach.
this book teaches statistics through puzzles, stories, visual aids, and real-world examples.
google is a master at creating data products.
if you want to create new information from existing, read codd.
a data application acquires its value from the data itself, and creates more data as a result.
statistics in a nutshell an introduction and reference for anyone with no previous background in statistics.
while we aren't drowning in a sea of data, we're finding that almost everything can (or has) been instrumented.
ben fry'sprocessing is the state of the art, particularly if you need to create animations that show how things change over time.
once you've parsed the data, you can start thinking about the quality of your data.
manufacturing and publicising statements that are designed to influence the level of trust in financial instruments is an entire industry in its own right.
whether that data is search terms, voice samples, or product reviews, the users are in a feedback loop in which they contribute to the products they use.
programming collective intelligence learn how to build web applications that mine the data created by people on the internet.
before it does anything else, itunes reads the length of every track, sends it to cddb, and gets back the track titles.
in this post, i examine the many sides of data science -- the technologies, the companies and the unique skill sets.
in addition to looking at profiles, linkedin's data scientists started looking at events that members attended.
they expose rich apis, and are designed for exploring and understanding the data rather than for traditional analysis and reporting.
once you've gotten some hints at what the data might be saying, you can follow it up with more detailed analysis.
ram has moved from $1,000/mb to roughly $25/gb -- a price reduction of about 40000, to say nothing of the reduction in size and increase in speed.
we've been writing some java and ruby code lately for in-house data analysis and even some basic segmentation of our huge inventory.
as with the number of followers on twitter, a "trending topics" report only needs to be current to within five minutes -- or even an hour.
physicists have a strong mathematical background, computing skills, and come from a discipline in which survival depends on getting the most from the data.
although r is an odd and quirky language, particularly to someone with a background in computer science, it comes close to providing "one stop shopping" for most statistical work.
kirk [ 2 june 2010 11:02 pm] thanks mike, "terabyte drives are consumer equipment" are the cord wood to my campfire.
programming collective intelligence learn how to build web applications that mine the data created by people on the internet.
at some point, traditional techniques for working with data run out of steam.
five years ago, in what is web 2.0, tim o'reilly said that "data is the next intel inside."
sites likeinfochimps and factual provide access to many large datasets, including climate data, myspace activity streams, and game logs from sporting events.
the problem with most data analysis algorithms is that they generate a set of numbers.
beautiful visualization this book demonstrates why visualizations are beautiful not only for their aesthetic design, but also for elegant layers of detail.
errors like this one have a direct impact on trustworthiness.
flu trends google was able to spot trends in the swine flu epidemic roughly two weeks before the center for disease control by analyzing searches that people were making in different regions of the country.
i think you mean that nasa's data screening delayed discovery of ozone depletion (as your links discuss), not global warming.
2) a person who creates the structure or map of information which allows others to find their personal paths to knowledge.
i do question the term data 'science', as in "data science enables the creation of data products.".
while we aren't drowning in a sea of data, we're finding that almost everything can (or has) been instrumented.
at ibm'smany eyes, many of the visualizations are full-fledged interactive applications.
many of these databases are the logical descendants of google'sbigtable and amazon's dynamo, and are designed to be distributed across many nodes, to provide "eventual consistency" but not absolute consistency, and to have very flexible schema.
the foreclosure data used in "data mashups in r " was posted on a public website by the philadelphia county sheriff's office.
it is crucial to "making data speak coherently."
a data application acquires its value from the data itself, and creates more data as a result.
to do it well you need to understand the grammatical structure of a job posting; you need to be able to parse the english.
managing sharding and replication across a horde of database servers is difficult and slow.
it has excellent graphics facilities; cran includes parsers for many kinds of data; and newer extensions extend r into distributed computing.
while that sounds like a simple task, the trick was disambiguating "apple" from many job postings in the growing apple industry.
i'm surprised that there are no comments, but 123 retweets already.
when you've just spent a lot of grant money generating data, you can't just throw the data out if it isn't as clean as you'd like.
they aren't well-behaved xml files with all the metadata nicely in place.
at ibm'smany eyes, many of the visualizations are full-fledged interactive applications.
at o'reilly, we frequently combine publishing industry data fromnielsen bookscan with our own sales data, publicly available amazon data, and even job data to see what's happening in the publishing industry.
if anything can be called a one-stop information platform, hadoop is it.
what differentiates data science from statistics is that data science is a holistic approach.
amazon'selastic mapreduce makes it much easier to put hadoop to work without investing in racks of linux machines, by providing preconfigured hadoop images for its ec2 clusters.
their business is fundamentally different from selling music, sharing music, or analyzing musical tastes (though these can also be "data products").
the importance of moore's law as applied to data isn't just geek pyrotechnics.
it's an excellent way to classify a few thousand data points at a cost of a few cents each.
we've all heard the joke that eating pickles causes death, because everyone who dies has eaten pickles.
precision has an allure, but in most data-driven applications outside of finance, that allure is deceptive.
a second best can be data sampling to reduce the analyzable data set to more tractable proportions.
cddb is a great example of data jiujitsu: identifying music by analyzing an audio stream directly is a very difficult problem (though not unsolvable -- seemidomi, for example).
beautiful data learn from the best data practitioners in the field about how wide-ranging -- and beautiful -- working with data can be.
relational databases are designed for consistency, to support complex transactions that can easily be rolled back if any one of a complex set of operations fails.
it's easer to consult with clients to figure out whether you're asking the right questions, and it's possible to pursue intriguing possibilities that you'd otherwise have to drop for lack of time.
it would have been easy to turn this into a high-ceremony development project that would take thousands of hours of developer time, plus thousands of hours of computing time to do massive correlations across linkedin's membership.
to do data conditioning, you have to be ready for whatever comes, and be willing to use anything from ancient unix utilities such asawk to xml parsers and machine learning libraries.
once you've gotten some hints at what the data might be saying, you can follow it up with more detailed analysis.
in hindsight, mapreduce seems like an obvious solution to google's biggest problem, creating large searches.
information platforms are similar to traditional data warehouses, but different.
it's not easy to get a handle on jobs in data science.
but hadoop (and particularly elastic mapreduce) make it easy to build clusters that can perform computations on long datasets quickly.
but the cddb staff used data creatively to solve a much more tractable problem that gave them the same result.
i have posted this article on cloud secuirty alliance group in linked in.
edward tufte'svisual display of quantitative information is the classic for data visualization, and a foundational text for anyone practicing data science.
you need some creativity for when the story the data is telling isn't what you think it's telling.
if anything can be called a one-stop information platform, hadoop is it.
faster computations make it easier to test different assumptions, different datasets, and different algorithms.
loss of privacy and the simultaneous rise of data engineering is no accident.
spell checking isn't a terribly difficult problem, but by suggesting corrections to misspelled searches, and observing what the user clicks in response, google made it much more accurate.
google isn't the only company that knows how to use data.
1) the individual who organizes the patterns inherent in data, making the complex clear.
then at books members had in their libraries.
mobile applications leave an even richer data trail, since many of them are annotated with geolocation, or involve video or audio, all of which can be mined.
hadoop processes data as it arrives, and delivers intermediate results in (near) real-time.
i love how "data science" is really taking on a life of its own now.
when you've just spent a lot of grant money generating data, you can't just throw the data out if it isn't as clean as you'd like.
if the problem involves human language, understanding the data adds another dimension to the problem.
the problem with most data analysis algorithms is that they generate a set of numbers.
as far image data sets are concerned , being multidimensional has a total mathematical basis for its processing and result retrievals.
traditional relational database systems stop being effective at this scale.
the more storage is available, the more data you will find to put into it.
it has a 5 mb capacity and it's stored in a cabinet roughly the size of a luxury refrigerator.
if you can split your task up into a large number of subtasks that are easily described, you can use mechanical turk's marketplace for cheap labor.
[22 march 2011 03:38 am] while data science is incredibly beneficial and helping to improve the standard of living of humanity, i'm not so sure that i go so far as to agree with hal varian that statistics is the next sexy job.
statistics plays a role in everything from traditional business intelligence (bi) to understanding how google's ad auctions work.
they can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions.
in addition to being physicists, mathematicians, programmers, and artists, they're entrepreneurs.
precision has an allure, but in most data-driven applications outside of finance, that allure is deceptive.
gracenote built a database of track lengths, and coupled it to a database of album metadata (track titles, artists, album titles).
mechanical turk is also an important part of the toolbox.
m s prasad [ 9 june 2010 01:40 am] great knowledge filled post and good coverage of tehnology.
if you have already reduced the set to 10,000 postings with the word "apple," paying humans $0.01 to classify them only costs $100.
google has indexed many, many websites about large snakes.
in addition to being physicists, mathematicians, programmers, and artists, they're entrepreneurs.
according to dj patil, chief scientist atlinkedin (@dpatil), the best data scientists tend to be "hard scientists," particularly physicists, rather than computer science majors.
[20 june 2011 12:45 am] very interesting post..i get a lot of information and insights here..
in hindsight, mapreduce seems like an obvious solution to google's biggest problem, creating large searches.
that arithmetic error really hurts.
the foreclosure data used in "data mashups in r " was posted on a public website by the philadelphia county sheriff's office.
the point is how one can get benefit from unstructured massive amount of data.
statistics plays a role in everything from traditional business intelligence (bi) to understanding how google's ad auctions work.
i want to know more about how video data is being analyzed.
mobile applications leave an even richer data trail, since many of them are annotated with geolocation, or involve video or audio, all of which can be mined.
if you've ever seen the html that's generated by excel, you know that's going to be fun to process.
once you've collected your training data (perhaps a large collection of public photos from twitter), you can have humans classify them inexpensively -- possibly sorting them into categories, possibly drawing circles around faces, cars, or whatever interests you.
many of the key hadoop developers have found a home atcloudera, which provides commercial support.
if you have already reduced the set to 10,000 postings with the word "apple," paying humans $0.01 to classify them only costs $100.
visualization is crucial to each stage of the data scientist.
data is indeed the new intel inside.
this book teaches statistics through puzzles, stories, visual aids, and real-world examples.
if you can split your task up into a large number of subtasks that are easily described, you can use mechanical turk's marketplace for cheap labor.
google, amazon, facebook, and linkedin have all tapped into their datastreams and made that the core of their success.
data science enables the creation of data products.
hadoop goes far beyond a simple mapreduce implementation (of which there are several); it's the key component of a data platform.
while this sounds simple enough, it's revolutionary: cddb views music as data, not as audio, and creates new value in doing so.
describing the data science group he put together at facebook (possibly the first data science group at a consumer-oriented web property), jeff hammerbacher said: ... on any given day, a team member could author a multistage processing pipeline in python, design a hypothesis test, perform a regression analysis over data samples with r, design and implement an algorithm for some data-intensive product or service in hadoop, or communicate the results of our analyses to other members of the organization3
they group together fundamentally dissimilar products by telling you what they aren't.
data science enables the creation of data products.
they come about because amazon understands that a book isn't just a book, a camera isn't just a camera, and a customer isn't just a customer; customers generate a trail of "data exhaust" that can be mined and put to use, and a camera is a cloud of data that can be correlated with the customers' behavior, the data they leave every time they visit the site.
more to the point, it's easy to notice that one advertisement forr in a nutshell generated 2 percent more conversions than another.
data science , as i understand has been a forte of statistics & mathematicians for long .
google'spagerank algorithm was among the first to use data outside of the page itself, in particular, the number of links pointing to a page.
nathan yau's flowingdata blog is a great place to look for creative visualizations.
her job as scientist at bit.ly is really to investigate the data that bit.ly is generating, and find out how to build interesting products from it.
statistics in a nutshell an introduction and reference for anyone with no previous background in statistics.
these recommendations are "data products" that help to drive amazon's more traditional retail business.
ram has moved from $1,000/mb to roughly $25/gb -- a price reduction of about 40000, to say nothing of the reduction in size and increase in speed.
to do data conditioning, you have to be ready for whatever comes, and be willing to use anything from ancient unix utilities such asawk to xml parsers and machine learning libraries.
joel [ 2 june 2010 09:25 pm] ...and still, to date, the best tool to mess around with large data sets is sas - been around for ages.
if you have a cd that's not in the database (including a cd you've made yourself), you can create an entry for an unknown album.
[21 july 2010 07:47 am] excellent article on new data management trends... traditional approaches to db and storage / analytics have to be changed, in order to cope with this huge evolution on data and stream decision analysis... olap, bi and traditional data mining have to be "updated" with new r&d tips... david alan christopher
this is facebook's play in the display advertising game, with their launch of the "i like" feature.
factual enlists users to update and improve its datasets, which cover topics as diverse as endocrinologists to hiking trails.
these features only require soft real-time; reports on trending topics don't require millisecond accuracy.
nathan yau's flowingdata blog is a great place to look for creative visualizations.
seems that as the web matures, so too does the tool sets and methodologies it uses to extract value.
it is crucial to "making data speak coherently."
making data tell its story a picture may or may not be worth a thousand words, but a picture is certainly worth a thousand numbers.
many sources of "wild data" are extremely messy.
the result was a valuable data product that analyzed a huge database -- but it was never conceived as such.
the importance of moore's law as applied to data isn't just geek pyrotechnics.
you can allocate and de-allocate processors as needed, paying only for the time you use them.
it would be nice if there was a standard set of tools to do the job, but there isn't.
but we've seen much bigger increases in storage capacity, on every level.
this graph shows the increase in cassandra jobs, and the companies listing cassandra positions, over time.
traditional data analysis has been hampered by extremely long turn-around times.
here's a few examples: google's breakthrough was realizing that a search engine could use input other than the text on the page.
it's reported that the discovery of ozone layer depletion was delayed becauseautomated data collection tools discarded readings that were too low 1.
there's a database behind a web front end, and middleware that talks to a number of other databases and data services (credit card processing companies, banks, and so on).
they accept all data formats, including the most messy, and their schemas evolve as the understanding of the data changes.
if you start a calculation, it might not finish for hours, or even days.
google, amazon, facebook, and linkedin have all tapped into their datastreams and made that the core of their success.
if you've ever used itunes to rip a cd, you've taken advantage of this database.
but old-stylescreen scraping hasn't died, and isn't going to die.
a new startup,riptano, provides commercial support.
jewel ward [21 june 2010 12:53 pm] i would add to this list of aspects of data science the ability to determine the best way to migrate, store, & archive data.
a new startup,riptano, provides commercial support.
making data tell its story isn't just a matter of presenting results; it involves making connections, then going back to other data sources to verify them.
try usinggoogle trends to figure out what's happening with thecassandra database or the python language, and you'll get a sense of the problem.
they expose rich apis, and are designed for exploring and understanding the data rather than for traditional analysis and reporting.
the problem of trust is interesting.
to store huge datasets effectively, we've seen a new breed of databases appear.
i bet this space will only grow as data becomes more and more important for organizations world-wide.
data is only useful if you can do something with it, and enormous datasets present computational problems.
the data exhaust you leave behind whenever you surf the web, friend someone on facebook, or make a purchase in your local supermarket, is all carefully collected and analyzed.
roger magoulas, who runs the data analysis group at o'reilly, was recently searching a database for apple job listings requiring geolocation skills.
humans tend to misinterpret many effects as causes because the effect can be seen while the cause doesn't exist as a singular event but is a complex web if feeding stimuli that need a complex web of receptive context to actually make something happen.
all human activity cannot be causally interpreted because social activity must be seen as complex adaptive systems that undergo continuous change that is hardly ever reflected in the data model and processing.
i love the examples of facebook, linkedin and amazon using patterns to suggest other relationships and cross sell.
whether you look at bits per gram, bits per dollar, or raw capacity, storage has more than kept pace with the increase of cpu speed.
they share the fact that their principal drivers are forms of pollution from human activities, but they are not the same in their impacts or the actions needed to mitigate them.
it was an agile, flexible process that built toward its goal incrementally, rather than tackling a huge mountain of data all at once.
disambiguation is never an easy task, but tools like thenatural language toolkit library can make it simpler.
traditional relational database systems stop being effective at this scale.
while that sounds like a simple task, the trick was disambiguating "apple" from many job postings in the growing apple industry.
data conditioning can involve cleaning up messy html with tools like beautiful soup, natural language processing to parse plain text in english and other languages, or even getting humans to do the dirty work.
i do feel a little uneasy, though, reading about the ability to extract all the types of information from raw data.
michael f. martin [ 2 june 2010 01:33 pm] accountants are a kind of data scientist too.
to store huge datasets effectively, we've seen a new breed of databases appear.
in addition to looking at profiles, linkedin's data scientists started looking at events that members attended.
when natural language processing fails, you can replace artificial intelligence with human intelligence.
increased storage capacity demands increased sophistication in the analysis and use of that data.
that joke doesn't work if you understand what correlation means.
hilary mason says that when she gets a new data set, she starts by making a dozen or more scatter plots, trying to get a sense of what might be interesting.
tracking links made google searches much more useful, and pagerank has been a key ingredient to the company's success.
you can allocate and de-allocate processors as needed, paying only for the time you use them.
working with data at scale we've all heard a lot about "big data," but "big" is really a red herring.
we've all heard a lot about "big data," but "big" is really a red herring.
cddb is a great success because their data model is simple and the data are really very limited, a few million rows.
